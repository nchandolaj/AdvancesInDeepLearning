# GH100 Processor

The GH100 is the flagship silicon die powering NVIDIA's "Hopper" architecture, which was released in 2022. It is a massive, monolithic processor designed specifically for high-performance computing (HPC) and the world's most demanding AI workloads, such as training Large Language Models (LLMs).

## 1. Naming Convention

NVIDIA's naming for data center hardware follows a specific two-part logic: 
* the **Silicon Name**, and
* the **Product Name**.

| Name Type | Example | Meaning|
|:-- |:-- |:-- |
|**Architecture** | **Hopper** | The overall design generation (named after computer scientist *Grace Hopper*).|
|**Silicon/Die** | **GH100** | **G** (Graphics) + **H** (Hopper) + **100** (Flagship tier). This refers to the physical chip itself.|
|**Product** | **H100** | The commercial name of the card you buy (e.g., H100 SXM5 or H100 PCIe).|

> **Important Distinction**: While the **GH100** chip has 144 Streaming Multiprocessors (SMs) physically etched onto it, the commercial **H100** product usually has some of these disabled (shipping with 132 SMs) to improve manufacturing yields.

## 2. Hardware Architecture Overview
The GH100 is built on a custom **TSMC 4N** process and contains over **80 billion transistors**. Its architecture is organized into a hierarchy of clusters designed for massive parallelism.

### Key Architectural Units:
* **GPC (Graphics Processing Clusters):** The GH100 contains **8 GPCs**. These are the largest functional blocks.
* **TPC (Texture Processing Clusters):** Each GPC contains **9 TPCs**.
* **SM (Streaming Multiprocessors):** Each TPC houses **2 SMs**. This brings the "full fat" silicon to **144 SMs** (though H100 products typically use 132).
* **Warps:** Within each SM, threads are grouped into "Warps" of **32** (i.e. 32 parallel threads). These are the smallest units of work scheduled by the hardware.
* **CUDA Cores:** Each SM contains 128 FP32 cores, totaling **18,432 CUDA cores** on a full chip. The individual processors that execute the instructions for each thread in a Warp.

### Breakthrough Technologies in GH100:
* **4th Gen Tensor Cores:** Optimized for deep learning, these now support **FP8 (8-bit floating point)**, which doubles the throughput for AI training compared to previous 16-bit formats.
* **Transformer Engine:** This is a specialized software/hardware hybrid that dynamically manages precision (switching between FP8 and FP16) to accelerate Transformer models (like GPT) without losing accuracy.
* **Distributed Shared Memory:** Allows SMs to share data directly with each other across a "Cluster" of SMs, reducing the need to write back to the slower global memory.
* **HBM3 Memory:** The GH100 was the first to use High Bandwidth Memory 3, providing up to **3 TB/s** of bandwidthâ€”a massive leap over the A100.

## 3. GH100 vs. Previous Gen (A100)
| Feature | A100 (Ampere) | GH100 (Hopper) |
| :--- | :--- | :--- |
| **Process** | TSMC 7nm | TSMC 4N (5nm class) |
| **Transistors** | 54.2 Billion | 80 Billion |
| **Max SMs** | 128 | 144 |
| **Max Bandwidth** | 2.0 TB/s | 3.35 TB/s |
| **New Features** | Multi-Instance GPU (MIG) | Transformer Engine, FP8 Support |

